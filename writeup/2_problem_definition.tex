\section{Problem Definition}
\label{sec:problem_definition}

A Multi-Objective Markov Decision Process (MOMDP) is a sequential decision process in which an agent controls a domain with a finite set of states. The actions the agent can perform in each state cause a stochastic transition to a successor state. This transition results in a reward, which consistes of a vector of values, each of which depends on the state transition and action. The process unfolds over a finite or infinite number of discrete time steps.  In a standard MDP, there is a single reward function and the goal is to maximize the expected cummulative discounted reward over the sequence of stages. MOMDPs present a more general model with multiple reward functions. We define below a variant of MOMDPs that we call Lexicographic MDP (LMDP), which extends MOMDPs with lexicographic preferences to also include conditional preferences and slack. We then introduce a Lexicographic Value Iteration (LVI) algorithm (Algorithm~\ref{alg:lvi}) which solves LMDPs.

\begin{definition}
    \label{def:lmdp}
    A \textbf{Lexicographic Markov Decision Process (LMDP)} is a represented by a 7-tuple $\langle S, A, T, \mathbf{R}, \delta, \mathcal{S}, o \rangle$:
    \begin{itemize}
        \item $S$ is a finite set of $n$ states, with initial state $s_0 \in S$
        \item $A$ is a finite set of $m$ actions
        \item $T : S \times A \times S \rightarrow [0, 1]$ is a state transition function which specifies the probability of transitioning from a state $s \in S$ to state $s' \in S$, given action $a \in A$ was performed; equivalently, $T(s, a, s') = Pr(s' | s, a)$
        \item $\mathbf{R} = [R_1, \ldots, R_k]^T$ is a vector of reward functions such that $\forall i \in K = \{1, \ldots, k\}$, $R_i : S \times A \times S \rightarrow \mathbb{R}$; each specifies the reward for being in a state $s \in S$, performing action $a \in A$, and transitioning to a state $s' \in S$, often written as $\mathbf{R}(s, a, s') = [R_1(s, a, s'), \ldots, R_k(s, a, s')]$
        \item $\delta = \langle \delta_1, \ldots, \delta_k \rangle$ is a tuple of slack variables such that $\forall i \in K$, $\delta_i \geq 0$
        \item $\mathcal{S} = \{S_1, \dots, S_\ell\}$ is a set which forms an $\ell$-partition over the state space $S$
        \item $o = \langle o_1, \ldots, o_\ell \rangle$ is a tuple of strict preference orderings such that $L = \{1, \ldots, \ell\}$, $\forall j \in L$, $o_j$ is a $k$-tuple ordering elements of $K$
    \end{itemize}
\end{definition}

We consider \emph{infinite horizon} LMDPs (i.e., $h = \infty$), with a \emph{discount factor} $\gamma \in [0, 1)$, due to space considerations. The finite horizon case follows in the natural way. A \emph{policy} $\pi : S \rightarrow A$ maps each state $s \in S$ to an action $a \in A$.

%We call $h \in \mathbb{N} \cup \{ \infty \}$ the \emph{horizon}, i.e., number of steps until the process terminates. The process may be defined as finite ($h < \infty$) or infinite ($h = \infty$). Over the iterations, rewards are discounted by \emph{discount factor} $\gamma$. For finite horizon problems, typically $\gamma = 1$; for infinite horizon problems, $\gamma \in [0, 1)$.
%A \emph{policy} $\pi : S \rightarrow A$ is a mapping from each state $s \in S$ to an action $a \in A$. In finite horizon problems, we have a sequence of policies $\langle \pi_1, \ldots, \pi_h \rangle$ representing a policy for each stage.

Let $\mathbf{V} = [V_1, \ldots, V_k]^T$ be a set of \emph{value functions}. Let each function $V_i^\pi : S \rightarrow \mathbb{R}$, $\forall i \in K$, represent the value of states $S$ following policy $\pi$. The stochastic process of MDPs enable us to represent this using the expected value over the reward for following the policy at each stage.
\begin{equation*}
    \mathbf{V}^\pi(s) = \mathbb{E} \Big[ \sum_{t=0}^\infty \gamma^t \mathbf{R}(s^t, \pi(s^t), s^{t+1}) \Big| s^0 = s, \pi \Big]
\end{equation*}

This allows us to recursively write the value of the state $s \in S$, given a particular policy $\pi$, in the following manner.
\begin{equation*}
    \mathbf{V}^\pi(s) = \sum_{s' \in S} T(s, \pi(s), s') (\mathbf{R}(s, \pi(s), s') + \gamma \mathbf{V}^\pi(s'))
\end{equation*}


\begin{algorithm}[t]
    \begin{algorithmic}[1]
        \caption{Lexicographic Value Iteration (LVI)}
        \label{alg:lvi}
        % \REQUIRE $???$: ???
        \STATE $V \leftarrow 0$
        \STATE $V' \leftarrow 0$
        \WHILE{$\|V - V'\|_\infty^{S} > \epsilon \frac{1 - \gamma}{\gamma}$}
            \STATE $V' \leftarrow V$
            \STATE $V^{fixed} \leftarrow V$
            \FOR{$j = 1, \ldots, \ell$}
                \FOR{$i = o_j(1), \ldots, o_j(k)$}
                    \WHILE{$\| V_i' - V_i \|_\infty^{\mathcal{S}_j} > \epsilon \frac{1 - \gamma}{\gamma}$}
                        \STATE $V_i'(s) \leftarrow V_i(s)$, $\forall s \in S_j$
                        \STATE $V_i(s) \leftarrow B_i V_i'(s)$, $\forall s \in S_j$
                    \ENDWHILE
                \ENDFOR
            \ENDFOR
        \ENDWHILE
        \RETURN $V'$
    \end{algorithmic}
\end{algorithm}


\subsection{Lexicographic Value Iteration}
\label{sec:lvi}

LMDPs lexicographically maximize $V_{o_j(i)}(s)$ over $V_{o_j(i+1)}(s)$, for all $i \in \{1, \ldots, k - 1\}$, $j \in L$, and $s \in S$, using $V_{oj{i+1}}$ to break ties. The model allows for slack as defined by $\eta_{o_j(i)} \geq 0$ (deviation from optimal for a single action change) and $\delta_{o_j(i)} \geq 0$ (deviation from the overall optimal value). As we show below, the classical value iteration algorithm~\cite{Bellman57} can be easily modified to solve MOMDPs with this preference characterization.

%% [[seems overly formal giving a number...]]
For the sake of readability, we use the following convention: Always assume that the ordering is present, unless otherwise stated. 
%% This notation vastly improves readability.
This allows us to omit the explicit ordering $o_j(\cdot)$ for subscripts, sets, etc. For example, $V_{i+1} \equiv V_{o_j(i+1)}$, and $\{1, \ldots, i - 1\}$ $\equiv$ $\{o_j(1), \ldots, o_j(i - 1)\}$.


%The algorithm begins by running value iteration for $V_1$ until convergence. This determines the subset of actions available to the next value function at each state. Then, value iteration is run on $V_2$ until convergence, which defines further restricted subset of actions for $V_3$. This process iterates until $V_k$ converges, at which point we have our final policy.

First, Equation~\ref{eq:value_of_state_action} defines $Q_i(s, a)$, the value of taking an action $a \in A$ in a state $s \in S$ according to objective $i \in K$.
\begin{equation}
    \label{eq:value_of_state_action}
    Q_i(s, a) = \sum_{s' \in S} T(s, a, s') (R_i(s, a, s') + \gamma V_i(s'))
\end{equation}

With this definition in place, we may define the aforementioned restricted set of actions for each state $s \in S$. For $i = 1$, let $A_1(s) = A$ and for all $i \in \{1, \ldots, k - 1\}$ let $A_{i+1}(s)$ be defined following Equation~\ref{eq:restricted_actions_set}.
\begin{equation}
    \label{eq:restricted_actions_set}
    A_{i+1}(s) = \{ a \in A_i(s) | \max_{a' \in A_i(s)} Q_i(s, a') - Q_i(s, a) \leq \eta_i \}
\end{equation}
For reasons explained in Section~\ref{sec:theoretical_analysis}, we let $\eta_i = (1 - \gamma) \delta_i$.

Finally, let Equation~\ref{eq:lvi_slack} below be the \emph{Bellman update equation} for MOMDPs with lexicographic reward preferences for $i \in K$, using slack $\delta_i \geq 0$, for all states $s \in S$. If $i > 1$, then we require $V_{i-1}$ to have converged for all states.
\begin{equation}
    \label{eq:lvi_slack}
    V_i(s) = \max_{a \in A_i(s)} Q_i(s, a)
\end{equation}

Within the algorithm, we leverage a modified value iteration with slack Bellman update equation (from Equation~\ref{eq:lvi_slack}) denoted as $B_i$. We either use $V_i$ for $s \in S_j \subseteq S$ or $V_i^{fixed}(s)$ for $s \in S \setminus S_j$, as shown in Equation~\ref{eq:lvi} below, with $[\cdot]$ denoting Iverson brackets.
\begin{align}
    B_i V_i'(s) &= \max_{a \in A_i(s)} \sum_{s' \in S} T(s, a, s') (R_i(s, a, s') + \gamma \bar{V}_i(s')) \label{eq:lvi} \\
    \bar{V}_i(s') &= V_i'(s') [s \in S_j] + V_i^{fixed}(s') [s \notin S_j] \label{eq:lvi_V_bar}
\end{align}
Also, in each infinity norm we denote the domain of the maximization such that $\| \cdot \|_\infty^Z = \max_{z \in Z} | \cdot |$.

