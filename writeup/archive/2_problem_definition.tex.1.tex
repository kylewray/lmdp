\section{Problem Definition}
\label{sec:problem_definition}

We present a formal model for multi-objective MDPs (MOMDPs) with a lexicographic ordering over the rewards' value functions. An MDP is a stochastic control process in which an agent exists in a set of states. The actions the agent performs in particular state each have a distribution over potential successor states. This transition results in a reward. The process operates over the state space for a finite or (virtually) infinite number of discrete time steps. The goal is to maximize an expected reward over the collection stages. Definition~\ref{def:mdp} formally states this process.
\begin{definition}
    \label{def:mdp}
    A \textbf{Multi-Objective Markov Decision Process (MOMDP) with lexicographic reward preferences} is a represented by a 4-tuple $\langle S, A, T, \mathbf{R} \rangle$ with:
    \begin{itemize}
        \item $S$ is a finite set of $n$ states, with initial state $s_0 \in S$
        \item $A$ is a finite set of $m$ actions
        \item $T : S \times A \times S \rightarrow [0, 1]$ is a state transition function which specifies the probability of transitioning from a state $s \in S$ to state $s' \in S$, given action $a \in A$ was performed; equivalently, we may write $T(s, a, s') = Pr(s' | s, a)$
        \item $\mathbf{R} = [R_1, \ldots, R_k ]^T$ is a vector of reward functions $R_i : S \times A \times S \rightarrow \mathbb{R}$, $\forall i \in \{1, \ldots, k\}$, each specifies the reward for being in a state $s \in S$, performing action $a \in A$, and transitioning to a state $s' \in S$, often written as $\mathbf{R}(s, a, s') = [R_1(s, a, s'), \ldots, R_k(s, a, s')]^T$
    \end{itemize}
\end{definition}

We call $h \in \mathbb{N}$ the \emph{horizon}, i.e., number of steps until the process terminates. The process may be defined as finite ($h < \infty$) or infinite ($h = \infty$). Over the iterations, rewards are discounted by \emph{discount factor} $\gamma$. For finite horizon problems, typically $\gamma = 1$; for infinite horizon problems, $\gamma \in [0, 1)$.

A \emph{policy} $\pi : S \rightarrow A$ is a mapping from each state $s \in S$ to an action $a \in A$. In finite horizon problems, we have a sequence of policies $\langle \pi_1, \ldots, \pi_h \rangle$ representing a policy for each stage.

Let $\mathbf{V} = [V_1, \ldots, V_k]^T$ be a set of \emph{value functions}. Let each function $V_k^\pi : S \rightarrow \mathbb{R}$, $\forall k \in \{1, \ldots, k\}$, represent the value of states $S$ following policy $\pi$. The stochastic process of MDPs enable us to represent this using the expected value over the reward for following the policy at each stage.
\begin{equation*}
    \mathbf{V}^\pi(s) = \mathbb{E} \Big[ \sum_{t=0}^{h-1} \gamma^t \mathbf{R}(s^t, \pi(s^t), s^{t+1}) \Big| s^0 = s, \pi \Big]
\end{equation*}

This allows us to recursively write the value of the state $s \in S$, given a particular policy $\pi$, in the following manner.
\begin{equation*}
    \mathbf{V}^\pi(s) = \sum_{s' \in S} T(s, \pi(s), s') (\mathbf{R}(s, \pi(s), s') + \gamma \mathbf{V}^\pi(s'))
\end{equation*}
% The final policy $\pi$ is computed using the value function.
% \begin{equation}
%     \pi(s) = \argmax_{a \in A} \mathbf{V}...
% \end{equation}


\subsection{Value Iteration}

We lexicographically prefer $V_k^\pi$ to $V_{k+1}^\pi$ for each $k$. This change requires a slight modification to value iteration. In particular, the $\max$ operator needs to be replaced with a \emph{lexicographic vector maximization} operator denoted $\lvmax$, which is defined in Equation~\ref{eq:lvmax} for a set $X$ and vector function $f : X \rightarrow \mathbb{R}^n$. Let $X_0 = X$ and $X_i = \argmax_{x \in X_{i-1}} f_i(x)$, for all $i \in \{1, \ldots, n-1\}$.
\begin{equation}
    \label{eq:lvmax}
    \lvmax_{x \in X} \mathbf{f}(x) = \begin{bmatrix}
            \max_{x \in X_0} f_1(x) \\
            \vdots \\
            \max_{x \in X_{n-1}} f_n(x)
        \end{bmatrix}
\end{equation}

With this definition, the Bellman update equation for MOMDPs with lexicographic reward preferences may be written as Equation~\ref{eq:lvmax_value_iteration} below for all states $s \in S$.
\begin{equation}
    \label{eq:lvmax_value_iteration}
    \mathbf{V}(s) = \lvmax_{a \in A} \sum_{s' \in S} T(s, a, s') (\mathbf{R}(s, a, s') + \gamma \mathbf{V}(s'))
\end{equation}

