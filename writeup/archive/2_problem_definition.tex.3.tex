\section{Problem Definition}
\label{sec:problem_definition}

We present a formal model for multi-objective MDPs (MOMDPs) with a lexicographic ordering over the rewards' value functions. An MDP is a stochastic control process in which an agent exists in a set of states. The actions the agent performs in particular state each have a distribution over potential successor states. This transition results in a reward. The process operates over the state space for a finite or (virtually) infinite number of discrete time steps. The goal is to maximize an expected reward over the collection stages. Definition~\ref{def:mdp} formally states this process.
\begin{definition}
    \label{def:mdp}
    A \textbf{Multi-Objective Markov Decision Process (MOMDP) with lexicographic reward preferences} is a represented by a 4-tuple $\langle S, A, T, \mathbf{R} \rangle$ with:
    \begin{itemize}
        \item $S$ is a finite set of $n$ states, with initial state $s_0 \in S$
        \item $A$ is a finite set of $m$ actions
        \item $T : S \times A \times S \rightarrow [0, 1]$ is a state transition function which specifies the probability of transitioning from a state $s \in S$ to state $s' \in S$, given action $a \in A$ was performed; equivalently, we may write $T(s, a, s') = Pr(s' | s, a)$
        \item $\mathbf{R} = [R_1, \ldots, R_k ]^T$ is a vector of reward functions $R_i : S \times A \times S \rightarrow \mathbb{R}$, $\forall i \in K$, with $K = \{1, \ldots, k\}$; each specifies the reward for being in a state $s \in S$, performing action $a \in A$, and transitioning to a state $s' \in S$, often written as $\mathbf{R}(s, a, s') = [R_1(s, a, s'), \ldots, R_k(s, a, s')]^T$
    \end{itemize}
\end{definition}

We call $h \in \mathbb{N} \cup \{ \infty \}$ the \emph{horizon}, i.e., number of steps until the process terminates. The process may be defined as finite ($h < \infty$) or infinite ($h = \infty$). Over the iterations, rewards are discounted by \emph{discount factor} $\gamma$. For finite horizon problems, typically $\gamma = 1$; for infinite horizon problems, $\gamma \in [0, 1)$.

A \emph{policy} $\pi : S \rightarrow A$ is a mapping from each state $s \in S$ to an action $a \in A$. In finite horizon problems, we have a sequence of policies $\langle \pi_1, \ldots, \pi_h \rangle$ representing a policy for each stage.

Let $\mathbf{V} = [V_1, \ldots, V_k]^T$ be a set of \emph{value functions}. Let each function $V_i^\pi : S \rightarrow \mathbb{R}$, $\forall i \in K$, represent the value of states $S$ following policy $\pi$. The stochastic process of MDPs enable us to represent this using the expected value over the reward for following the policy at each stage.
\begin{equation*}
    \mathbf{V}^\pi(s) = \mathbb{E} \Big[ \sum_{t=0}^{h-1} \gamma^t \mathbf{R}(s^t, \pi(s^t), s^{t+1}) \Big| s^0 = s, \pi \Big]
\end{equation*}

This allows us to recursively write the value of the state $s \in S$, given a particular policy $\pi$, in the following manner.
\begin{equation*}
    \mathbf{V}^\pi(s) = \sum_{s' \in S} T(s, \pi(s), s') (\mathbf{R}(s, \pi(s), s') + \gamma \mathbf{V}^\pi(s'))
\end{equation*}
% The final policy $\pi$ is computed using the value function.
% \begin{equation}
%     \pi(s) = \argmax_{a \in A} \mathbf{V}...
% \end{equation}


\subsection{Lexicographic Value Iteration}

We lexicographically prefer $V_i^\pi$ to $V_{i+1}^\pi$ for each $i \in K$. We are able to rewrite value iteration to solve multi-objective MDPs in this manner. In particular, the $\max$ operator needs to be replaced with a \emph{lexicographic vector maximization} operator, denoted $\lvmax$, which is defined in its general form in Definition~\ref{def:lvmax} below.

\begin{definition}
\label{def:lvmax}
The \textbf{lexicographic vector maximization} operator $\lvmax$ is defined in Equation~\ref{eq:lvmax} below. First, let $X$ be a set and $f : X \rightarrow \mathbb{R}^k$ be a vector function. Let $\delta = \langle \delta_1, \ldots, \delta_k \rangle$ be a tuple of slack variables, with $\delta_i \geq 0$ for all $i \in K$. Next, let us define a reduction function $r : K \times \mathcal{P}(X) \times \mathbb{R}^+ \rightarrow \mathcal{P}(X)$ as in Equation~\ref{eq:lvmax_r}, with $j \in K$, $Y \subseteq X$, and $\xi \in \mathbb{R}^+$.
\begin{equation}
    \label{eq:lvmax_r}
    r(j, Y, \xi) = \{y \in Y | \max_{y' \in Y} f_j(y') - f_j(y) \leq \xi \}
\end{equation}
Let $X_1 = X$, $X_i = r(i, X_{i-1}, \mathbf{\delta})$ for all $i \in \{2, \ldots, k\}$, and $X^* = X_k$. Similarly, let $X_1^* = X^*$ and $X_i^* = r(i, X_{i-1}^*, \mathbf{0})$.
\begin{equation}
    \label{eq:lvmax}
    \lvmax_{x \in X^*} \mathbf{f}(x) = \begin{bmatrix}
            \max_{x \in X_1^*} f_1(x) \\
            \vdots \\
            \max_{x \in X_k^*} f_k(x)
        \end{bmatrix}
\end{equation}
Note that the special case in which $\delta_i = 0$ reduces $i$'s set definition $X_i = \argmax_{x \in X_i} f_i(x)$.
\end{definition}

With this definition, the \emph{Bellman update equation} for MOMDPs with lexicographic reward preferences may be written as Equation~\ref{eq:lvmax_value_iteration} below for all states $s \in S$.
\begin{equation}
    \label{eq:lvmax_value_iteration}
    \mathbf{V}(s) = \lvmax_{a \in A} \sum_{s' \in S} T(s, a, s') (\mathbf{R}(s, a, s') + \gamma \mathbf{V}(s'))
\end{equation}
