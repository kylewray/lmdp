\section{Introduction}
\label{sec:introduction}

% Introduce the premise:
% 1. Interesting / prevalent research area.
% 2. Example problem domains.
% 3. Some of the challenges, hinting that our algorithm addresses these.
% 4. Conclude with a one or two sentence description of our algorithm.
Stochastic planning problems designed to optimize multiple objectives are widespread within numerous domains such as smart homes and commercial buildings~\cite{Kwak12-SAVES}, reservoir water control~\cite{Castelletti08-WaterReservoirControl}, and autonomous robotics~\cite{Mouaddib04-MultiObjectivePathPlanning,Calisi07-MultiObjectiveExplorationAndSearchRobots}. 
Current approaches often use a scalarization function and a weight vector to project the multi-objective problem to a single-objective problem~\cite{Roijers13-SurveyMultiObjective,Natarajan05-DynamicPreferencesMCRL,Perny10-FindingCompromiseSolutionsMOMDPs,Perny13-LorenzOptimalSolutionsMOMDPs}. While these approaches leverage effectively the vast existing work on single-objective optimization, they have several drawbacks.  First, choosing a projection is often too onerous to use in practice since there are many viable Pareto optimal solutions to the original multi-objective problem, making it hard to visualize and analyze alternative solutions. Often there is no clear way to prefer one over another. In some cases, a simple lexicographic order exists among the objectives, for example using plan safety as primary criterion and cost as secondary. But lexicographic order of objectives can be too rigid, not allowing any tradeoffs between objectives (e.g., a large reduction in costs for a minimal reduction in safety).

Recent work by~\citeauthor{Mouaddib04-MultiObjectivePathPlanning} used a strict lexicographic preference ordering for multi-objective MDPs~\cite{Mouaddib04-MultiObjectivePathPlanning}. Others have also developed lexicographic orderings over value functions, calling this technique \emph{ordinal dynamic programming} \cite{Mitten74-PreferenceOrderDynamicProgramming,Sobel75-OrdinalDynamicProgramming}. \citeauthor{Mitten74-PreferenceOrderDynamicProgramming} assumed a specific preference ordering over outcomes for a finite horizon MDP; \citeauthor{Sobel75-OrdinalDynamicProgramming} extended this model to infinite horizon MDPs. Ordinal dynamic programming has been explored under reinforcement learning~\cite{Gabor98-MultiObjectiveReinforcementLearning,Natarajan05-DynamicPreferencesMCRL}, with the notion of a minimum criterion value, distinct from slack. 

We propose a natural extension of sequential decision making with lexicographic order by introducing two added model components: conditioning and slack.  Conditioning allows the lexicographic order to depend on certain state variables.  Slack allows a small deviation from the optimal value of a primary variable so as to improve secondary value functions.  The added flexibility is essential to cature practical preferences in many domains.  For example, in manufacturing, there is always a tradeoff among cost, quality, and time. In critical states of the manufacturing process, one may prefer to optimize quality with no slack, whereas in less important states one may optimize cost, but allow for some slack in order to improve time and quality. 

Our interest in developing this model stems from work on planning for semi-autonomous driving.  Consider a car that can operate autonomously under certain conditions, for example maintaining safe speed and distance from other vehicles on a highway.  All other roiad conditions require manual driving. A driver may want to minimize both the time needed to reach the destination and the effort associated with manual driving. The concepts of conditional preference and slack are quite valuable in defining the overall objective. To ensure safety, if the driver is tired, then selecting roads which are autonomous-capable are preferred without any margin of slack; however, if the driver is not tired, then roads which optimize travel time are preferred, perhaps with some slack to allow for long-distance, autonomous-capable highways. We focus on this domain for the remainder of the paper.


%% [[ a bit misplaced?]]
%Within the context of scalarization methods, Lorenz dominance has been used with weighted value functions to favor more ``fair'' values~\cite{Perny13-LorenzOptimalSolutionsMOMDPs}.

The general use of preference decomposition is popular within the field, as found in Generalized Additive Decomposable (GAI) networks~\cite{Gonzales11-DecisionMakingMultipleObjectivesGAINetworks} or Conditional Preference Networks (CP-Nets)~\cite{Boutilier04-CPNets}. Constrained MDPs (CMDPs) can also capture this preference structure, as well as slack, and are potentially a more general representation than LMDPs~\cite{Altman99-CMDPs}; however, to our knowledge lexicographic preferences have not been explored within the model. Various other forms of slack are also commonly found in the literature~\cite{Gabor98-MultiObjectiveReinforcementLearning}. Combining these ideas, LMDP and LVI naturally encapsulates many problem domains.


%%% [[not needed?]] We introduce the LMDP model and LVI algorithm that generalize these previous methods with our formulation of slack variables and conditional state-based preferences. Additionally, we present experimental evidence, which demonstrates its efficacy in an applied setting. To the best of our knowledge, no other model or algorithm allows for lexicographic state-based conditional preferences or our formulation of slack.

Our primary contributions include formulating the Lexicographic MDP (LMDP) model and the corresponding Lexicographic Value Iteration (LVI) algorithm. They generalize the previous methods mentioned above with our formulation of slack variables and conditional state-based preferences. We also provide an interesting connection between decision theory and game theory, in addition to a statement for the uniqueness of LMDP policies with respect to linearly weighted scalarization function policies. Furthermore, we introduce a new benchmark problem for multi-objective research: semi-autonomous driving. We implement general tools to experiment in this domain, leveraging the OpenStreetMap (OSM) package--a collaborative project to create a free editable map of the world.  Finally, we employ GPU-based optimization to implement our algorithm and show its general benefits for Value Iteration (VI) in MDPs.

Section~\ref{sec:problem_definition} states the LMDP problem definition. Section~\ref{sec:theoretical_analysis} presents our main convergence results, bound on slack, and an interesting relation to game theory. Section~\ref{sec:experimentation} discusses our experimentats within the context of semi-autonomous driving. Finally, Section~\ref{sec:conclusion} concludes with a final discussion of LMDPs and LVI.

% Discuss previous work on multi-objective optimization. Each time, state in 1 sentence what the algorithm does, then "but" with how our algorithm differs.
%Scalarization either assumes the weights are known, which produces a single policy, or allows for the selection of weights, which produces a set of possible Pareto optimal policies~\cite{Roijers13-SurveyMultiObjective}. In both cases, the designer must also select a linear or monotonically increasing scalarization function. We instead explore a lexicographic approach that allows for flexibility with slack. Since the problem domain is assumed to be known, it is natural to prescribe a state-dependent ordering over the rewards. Each action taken optimizes the value functions in order, only referring to the subsequent one if multiple actions exist which yield the optimal value, while allowing for some slack.

% Discuss previous work on the use of lexicographic orderings in MDPs, each time stating in 1 sentence what the approach was and how ours differs.


%\citeauthor{Barbara88-MaxminLeximax} characterized an operator called $\leximin$ within an economics context, which is closely related to $\lvmax$, except that the ordering is slightly different and does not include slack variables~\cite{Barbara88-MaxminLeximax}. Since its inception, it has enjoyed use outside the domain of MDPs by other economics researchers~\cite{Bossert94-RankingOpportunitySets,Fargier05-QuantitativeDecisionMaking,Arlegi05-FreedomOfChoice}.

%Our algorithm can capture the strict avoidance of dead ends as well as loops. Interestingly,~\citeauthor{Kolobov12-TheoryGoalOrientedMDPsDeadEnds} showed that unavoidable dead ends can be represented by a ``price''~\cite{Kolobov12-TheoryGoalOrientedMDPsDeadEnds}. The resulting formulation resembles a specific, non-slack variable version of $\lvmax$ value iteration.

%A solid survey of the approaches used to solve MOMDPs is provided by~\citeauthor{Roijers13-SurveyMultiObjective}~\shortcite{Roijers13-SurveyMultiObjective}, but other models exist. Constrained MDPs (CMDPs) are another formulation of MOMDPs, but to our knowledge no one has explored lexicographic preferences over rewards in this domain~\cite{Altman99-CMDPs}. Additionally,~\citeauthor{Gonzales11-DecisionMakingMultipleObjectivesGAINetworks} used Generalized Additive Decomposable (GAI) networks to capture the decomposability of the utility functions to model preferences~\cite{Gonzales11-DecisionMakingMultipleObjectivesGAINetworks}.

