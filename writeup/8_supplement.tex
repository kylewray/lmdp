\section*{Supplement to Submission \#}
\label{sec:supplement}

This section includes supplemental material for the paper.

Proof of {\bf Proposition~\ref{prop:lvi_convergence}}:
\begin{proof}
Expanding upon Proposition~\ref{prop:lvi_contraction}, for all $i \in K$, by definition of a contraction map, $B_i$ admits at most one fixed point. By \emph{Banach's fixed point theorem}, since $M_i$ is a complete metric space and $B_i$ is a contraction map on $Y_i$, $B_i$ admits a unique fixed point $V_i^* \in Y_i$. Therefore, the final $V^*$ is a unique fixed point in the space over all value functions over $i \in K$ and $s \in S_j$.

Finally, a corollary of Banach's fixed point theorem is that the speed of convergence to within $\epsilon > 0$ of the fixed point $x^*$ is known (using the generic notation from above for a metric space).
\begin{align*}
    d(x^*, x_{t+1}) &\leq \frac{\alpha}{1 - \alpha} d(x_{t+1}, x_t) \\
    \| V_i^* - V_i^{t+1}\|_\infty^{S_j} &\leq \frac{\gamma}{1 - \gamma} \| V_i^{t+1} - V_i^t \|_\infty^{S_j}
\end{align*}

Since we want the distance from the fixed point $V_i^*$ to be $\epsilon$, we may rewrite the equation accordingly.
\begin{align*}
    \epsilon &\leq \frac{\gamma}{1 - \gamma} \| V_i^{t+1} - V_i^t \|_\infty^{S_j} \\
    \epsilon \frac{1 - \gamma}{\gamma} &\leq \| V_i^{t+1} - V_i^t \|_\infty^{S_j}
\end{align*}

The above equation states that we are at least $\epsilon$ (or more) away from the fixed point when the maximum difference (over the states) between iterations satisfies the inequality. Therefore, we flip the inequality to create a convergence criterion, which ensures that we are $\epsilon$ or less from the fixed point.

\begin{equation*}
    \quad \| V_i^{t+1} - V_i^t \|_\infty^{S_j} < \epsilon \frac{1 - \gamma}{\gamma}
\end{equation*}
\end{proof}


Proof of {\bf Proposition~\ref{prop:lvi_uniqueness}}:
\begin{proof}
(a) We now prove that for $w_1 < 1/3$ and $w_2 > 2/3$, $\pi_\mathbf{w}(s_1) = leave$ and $\pi_\mathbf{w}(s_2) = stay$. Proof by induction on $t$.

\emph{Base Case:} $t = 1$.
\begin{align*}
    Q_\mathbf{w}^1(s_1, stay) &= 2 w_1 + \gamma V_\mathbf{w}^0(s_1) \\
        &< w_2 + \gamma V_\mathbf{w}^0(s_2) = Q_\mathbf{w}^1(s_1, leave) \\
    Q_\mathbf{w}^1(s_2, stay) &= w_2 + \gamma V_\mathbf{w}^0(s_2) \\
        &> 2 w_1 + \gamma V_\mathbf{w}^0(s_1) = Q_\mathbf{w}^1(s_2, leave) \\
    \Rightarrow \quad \quad &\pi_\mathbf{w}^1(s_1) = leave \quad \quad \pi_\mathbf{w}^1(s_2) = stay \\
    \Rightarrow \quad \quad &V_\mathbf{w}^1(s_1) = w_2 \quad \quad V_\mathbf{w}^1(s_2) = w_2
\end{align*}
Thus, the base case holds true. Note: Both value functions have the same value.

\emph{Induction Step:} Assume true for $t = T$, must show for $t = T + 1$.
\begin{align*}
    Q_\mathbf{w}^{T+1}&(s_1, stay) = 2 w_1 + \gamma V_\mathbf{w}^T(s_1) = 2 w_1 + w_2 \sum_{t=0}^{T-1} \gamma^t \\
        &< w_2 + \gamma V_\mathbf{w}^T(s_2) = w_2 \sum_{t=0}^T \gamma^t = Q_\mathbf{w}^{T+1}(s_1, leave) \\
    Q_\mathbf{w}^{T+1}&(s_2, stay) = w_2 + \gamma V_\mathbf{w}^T(s_2) = w_2 \sum_{t=0}^T \gamma^t \\
        &> 2 w_1 + \gamma V_\mathbf{w}^T(s_1) = 2 w_1 + w_2 \sum_{t=0}^{T-1} \gamma^t \\
        &= Q_\mathbf{w}^{T+1}(s_2, leave) \\
    \Rightarrow \quad \quad &\pi_\mathbf{w}^{T+1}(s_1) = leave \quad \quad \pi_\mathbf{w}^{T+1}(s_2) = stay \\
    \Rightarrow \quad \quad &V_\mathbf{w}^{T+1}(s_1) = w_2 \sum_{t=0}^T \gamma^t \quad \quad V_\mathbf{w}^{T+1}(s_2) = w_2 \sum_{t=0}^T \gamma^t 
\end{align*}
By induction, $\forall t \in \mathbb{N}$, $\pi_\mathbf{w}^t(s_1) = leave$ and $\pi_\mathbf{w}^t(s_2) = stay$ for weights $w_1 < 1/3$ and $w_2 > 2/3$.

(b) Apply the same logic for $w_1 > 1/3$ and $w_2 < 2/3$ to obtain the reverse policy: $\pi_\mathbf{w}(s_1) = stay$ and $\pi_\mathbf{w}(s_2) = leave$.

(c) Ambiguity exists at $w_1 = 1/3$ and $w_2 = 1/3$, wherein we must employ a tie-breaking rule. Under this scenario, we can construct a policy such that $\pi_\mathbf{w}(s_1) = \pi_\mathbf{w}(s_2) = stay$.

(d) Note that (a) and (b) are reversed for states $s_3$ and $s_4$ by applying the exact same logic again for these states. This means that for $w_1 < 2/3$ and $w_2 < 1/3$ it implies $\pi_\mathbf{w}(s_3) = leave$ and $\pi_\mathbf{w}(s_4) = stay$; for $w_1 > 2/3$ and $w_2 < 1/3$ it implies $\pi_\mathbf{w}(s_3) = stay$ and $\pi_\mathbf{w}(s_4) = leave$. Similar to (c), only with the weights defined as $w_1 = 2/3$ and $w_2 = 1/3$, ambiguity allows for a tie-breaking rule to produce $\pi_\mathbf{w}(s_3) = \pi_\mathbf{w}(s_3) = stay$.

(e) Combine (a)-(d), realizing that for states $S = \{s_1, s_2, s_3, s_4\}$, no weight exists to produce the policy $\pi_\mathbf{w}(s) = stay$ for all $s \in S$.

(f) Must show that LVI in an LMDP using these states, actions, transitions, and rewards can return the policy $\pi(s) = stay$ for all $s \in S$. Let $\mathcal{S} = \{S_1, S_2\}$, $S_1 = \{s_1, s_3\}$, $S_2 = \{s_2, s_4\}$, $o = \langle o_1, o_2 \rangle$, $o_1 = \langle 1, 2 \rangle$, $o_2 = \langle 2, 1 \rangle$, and $\delta = \langle 0, 0, \rangle$. Proof by induction on $t$.

\emph{Base Case:} $t = 1$.
\begin{align*}
    Q^1&(s_1, stay) = 2 + \gamma V_1^0(s_1) \\
        &> 0 + \gamma V_1^0(s_2) = Q^1(s_1, leave) \\
    Q^1&(s_2, stay) = 1 + \gamma V_2^0(s_2) \\
        &> 0 + \gamma V_2^0(s_1) = Q^1(s_2, leave) \\
    Q^1&(s_3, stay) = 1 + \gamma V_1^0(s_3) \\
        &> 0 + \gamma V_1^0(s_4) = Q^1(s_3, leave) \\
    Q^1&(s_4, stay) = 2 + \gamma V_2^0(s_4) \\
        &> 0 + \gamma V_2^0(s_3) = Q^1(s_4, leave) \\
    \Rightarrow \quad &\quad \forall s \in S, \quad A_2^1(s) = \{stay\} \\
    \Rightarrow \quad &\quad \forall s \in S, \quad \pi^1(s) = stay \\
    \Rightarrow \quad &\quad V_1(s_1) = V_2(s_4) = 2 \\
    \Rightarrow \quad &\quad V_1(s_2) = V_2(s_3) = 1
\end{align*}
Thus, the base case holds true.

\emph{Induction Step:} Assume true for $t = T$, must show for $t = T + 1$.
\begin{align*}
    Q^{T+1}&(s_1, stay) = 2 + \gamma V_1^T(s_1) = 2 \sum_{t=0}^T \gamma^t \\
        &> 0 + \gamma V_1^T(s_2) = 0 = Q^{T+1}(s_1, leave) \\
    Q^{T+1}&(s_2, stay) = 1 + \gamma V_2^T(s_2) = \sum_{t=0}^T \gamma^t \\
        &> 0 + \gamma V_2^T(s_1) = 0 = Q^{T+1}(s_2, leave) \\
    Q^{T+1}&(s_3, stay) = 1 + \gamma V_1^T(s_3) = \sum_{t=0}^T \gamma^t \\
        &> 0 + \gamma V_1^T(s_4) = 0 = Q^{T+1}(s_3, leave) \\
    Q^{T+1}&(s_4, stay) = 2 + \gamma V_2^T(s_4) = 2 \sum_{t=0}^T \gamma^t \\
        &> 0 + \gamma V_2^T(s_3) = 0 = Q^{T+1}(s_4, leave) \\
    \Rightarrow \quad &\quad \forall s \in S, \quad A_2^T(s) = \{stay\} \\
    \Rightarrow \quad &\quad \forall s \in S, \quad \pi^T(s) = stay \\
    \Rightarrow \quad &\quad V_1(s_1) = V_2(s_4) = 2 \sum_{t=0}^T \gamma^t \\
    \Rightarrow \quad &\quad V_1(s_2) = V_2(s_3) = \sum_{t=0}^T \gamma^t
\end{align*}
By induction, $\forall t \in \mathbb{N}$, $\pi^t(s_1) = stay$ and $\pi^t(s_2) = stay$.

Therefore, by (e) and (f), we have shown that no weights exist such that VI yields the policy produced by LVI.
\end{proof}

